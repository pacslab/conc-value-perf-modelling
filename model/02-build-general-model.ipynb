{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Building The General Model\n",
    "\n",
    "Using the analysis created by the previous sections, we will try to build and solve the\n",
    "general model for concurrency value autoscaling.\n",
    "\n",
    "\n",
    "One important thing to note here is that the general model is evaluated every 2 seconds\n",
    "because of the inherent autoscaling evaluation period of 2 seconds in knative. As a result\n",
    "we need to convert the rates of provisioning and deprovisioning of containers from\n",
    "time rates into probability rates (from CTMC to DTCM parameters)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# imports\n",
    "\n",
    "# important libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import scipy as sp\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "# for better printing of variables\n",
    "from IPython.display import display\n",
    "\n",
    "# custom imports\n",
    "from concperf import single_model, general_model\n",
    "from concperf import utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update configuration dictionary for each instance count\n",
    "def update_config(config):\n",
    "    config['arrival_rate_server'] = config['arrival_rate_total'] / config['instance_count']\n",
    "    config['base_service_time'] = config['base_service_time_ms'] / 1000\n",
    "\n",
    "model_config = {\n",
    "    # 'instance_count' should be added for each state\n",
    "    'max_conc': 10,\n",
    "    'arrival_rate_total': 5,\n",
    "    'alpha': 0.11,\n",
    "    'base_service_time_ms': 1154,\n",
    "    'max_container_count': 25,\n",
    "    'target_conc': 0.7, # assumes target utilization\n",
    "    'max_scale_up_rate': 1000, # from N to 1000*N at most\n",
    "    'max_scale_down_rate': 2, # from N to N/2 at most\n",
    "    'stable_conc_avg_count': 300, # number of times monitored concurrency will be averaged in stable mode\n",
    "    'autoscaling_interval': 2, # amount of time between autoscaling evaluations\n",
    "    'provision_rate_base': 1,\n",
    "    'deprovision_rate_base': 2,\n",
    "}\n",
    "\n",
    "# test\n",
    "# model_config = {\n",
    "#     # 'instance_count' should be added for each state\n",
    "#     'max_conc': 10,\n",
    "#     'arrival_rate_total': .001,\n",
    "#     'alpha': 0.11,\n",
    "#     'base_service_time_ms': 1154,\n",
    "#     'max_container_count': 25,\n",
    "#     'target_conc': 0.7, # assumes target utilization\n",
    "#     'max_scale_up_rate': 1000, # from N to 1000*N at most\n",
    "#     'max_scale_down_rate': 2, # from N to N/2 at most\n",
    "#     'stable_conc_avg_count': 300, # number of times monitored concurrency will be averaged in stable mode\n",
    "#     'autoscaling_interval': 2, # amount of time between autoscaling evaluations\n",
    "#     'provision_rate_base': 1,\n",
    "#     'deprovision_rate_base': 2,\n",
    "# }\n",
    "\n",
    "single_coder = single_model.StateCoder(config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of states: 676\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "[(0, 0),\n (0, 1),\n (0, 2),\n (0, 3),\n (0, 4),\n (0, 5),\n (0, 6),\n (0, 7),\n (0, 8),\n (0, 9)]"
     },
     "metadata": {}
    }
   ],
   "source": [
    "general_state_coder = general_model.StateCoder(model_config)\n",
    "print('Number of states:', general_state_coder.get_state_count())\n",
    "display(general_state_coder.get_state_list()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.13533528, 0.11701964, 0.10118276, 0.08748916, 0.07564879,\n",
       "       0.06541084, 0.05655845, 0.04890409, 0.04228564, 0.27016534])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "probs = utility.get_trans_probs(10, transition_rate_base=1, max_t=model_config['autoscaling_interval'])\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(array([1, 2, 3]), array([0.13533528, 0.11701964, 0.74764507]))"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(array([3, 2, 1]), array([0.01831564, 0.01798018, 0.96370418]))"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# NOTE: This part assumes delay center to allow scalability\n",
    "# TODO: Allow changing delay center to single server queue with config\n",
    "display(general_model.get_trans_probabilities(ready_count=1, ordered_count=3, config=model_config))\n",
    "display(general_model.get_trans_probabilities(ready_count=3, ordered_count=1, config=model_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(array([1, 0]), array([0.01831564, 0.98168436]))"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display(general_model.get_trans_probabilities(ready_count=1, ordered_count=0, config=model_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/26 [00:00<?, ?it/s]new order calculation took 1.2100200653076172 seconds for 1 instances\n",
      "  8%|▊         | 2/26 [00:02<00:24,  1.02s/it]new order calculation took 1.3199892044067383 seconds for 2 instances\n",
      " 12%|█▏        | 3/26 [00:04<00:34,  1.49s/it]new order calculation took 1.069002628326416 seconds for 3 instances\n",
      " 15%|█▌        | 4/26 [00:05<00:34,  1.57s/it]new order calculation took 0.6980326175689697 seconds for 4 instances\n",
      " 19%|█▉        | 5/26 [00:07<00:30,  1.47s/it]new order calculation took 0.6529924869537354 seconds for 5 instances\n",
      " 23%|██▎       | 6/26 [00:08<00:27,  1.39s/it]new order calculation took 0.681995153427124 seconds for 6 instances\n",
      " 27%|██▋       | 7/26 [00:09<00:25,  1.33s/it]new order calculation took 0.4660027027130127 seconds for 7 instances\n",
      " 31%|███       | 8/26 [00:10<00:22,  1.22s/it]new order calculation took 0.44997596740722656 seconds for 8 instances\n",
      " 35%|███▍      | 9/26 [00:11<00:19,  1.13s/it]new order calculation took 0.43793344497680664 seconds for 9 instances\n",
      " 38%|███▊      | 10/26 [00:12<00:16,  1.05s/it]new order calculation took 0.4230184555053711 seconds for 10 instances\n",
      " 42%|████▏     | 11/26 [00:13<00:14,  1.01it/s]new order calculation took 0.37299275398254395 seconds for 11 instances\n",
      " 46%|████▌     | 12/26 [00:14<00:13,  1.07it/s]new order calculation took 0.3760037422180176 seconds for 12 instances\n",
      " 50%|█████     | 13/26 [00:14<00:11,  1.11it/s]new order calculation took 0.3399660587310791 seconds for 13 instances\n",
      " 54%|█████▍    | 14/26 [00:15<00:10,  1.16it/s]new order calculation took 0.332000732421875 seconds for 14 instances\n",
      " 58%|█████▊    | 15/26 [00:16<00:09,  1.20it/s]new order calculation took 0.3200078010559082 seconds for 15 instances\n",
      " 62%|██████▏   | 16/26 [00:17<00:08,  1.23it/s]new order calculation took 0.3370330333709717 seconds for 16 instances\n",
      " 65%|██████▌   | 17/26 [00:17<00:07,  1.24it/s]new order calculation took 0.29197216033935547 seconds for 17 instances\n",
      " 69%|██████▉   | 18/26 [00:18<00:06,  1.28it/s]new order calculation took 0.2919638156890869 seconds for 18 instances\n",
      " 73%|███████▎  | 19/26 [00:19<00:05,  1.29it/s]new order calculation took 0.30699920654296875 seconds for 19 instances\n",
      " 77%|███████▋  | 20/26 [00:20<00:04,  1.23it/s]new order calculation took 0.48699522018432617 seconds for 20 instances\n",
      " 81%|████████  | 21/26 [00:21<00:04,  1.14it/s]new order calculation took 0.3110325336456299 seconds for 21 instances\n",
      " 85%|████████▍ | 22/26 [00:22<00:03,  1.12it/s]new order calculation took 0.294020414352417 seconds for 22 instances\n",
      " 88%|████████▊ | 23/26 [00:23<00:02,  1.08it/s]new order calculation took 0.3649871349334717 seconds for 23 instances\n",
      " 92%|█████████▏| 24/26 [00:24<00:01,  1.03it/s]new order calculation took 0.28899455070495605 seconds for 24 instances\n",
      " 96%|█████████▌| 25/26 [00:25<00:01,  1.00s/it]new order calculation took 0.24802923202514648 seconds for 25 instances\n",
      "100%|██████████| 26/26 [00:26<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "# find parameters for different configuration\n",
    "\n",
    "# TODO: do something about instance count of zero\n",
    "\n",
    "state_count = general_state_coder.get_state_count()\n",
    "general_Q = np.zeros((state_count, state_count))\n",
    "\n",
    "for ready_inst_count in tqdm(range(0, model_config['max_container_count']+1)):\n",
    "    # add instance count to config\n",
    "    model_config.update({\n",
    "        'instance_count': max(ready_inst_count, 1), # for 0 ready containers, solve CC with single server\n",
    "    })\n",
    "\n",
    "    # update the config\n",
    "    update_config(model_config)\n",
    "\n",
    "    # calculate and show Q\n",
    "    single_Q = single_model.get_single_container_q(single_coder, config=model_config)\n",
    "    # display(pd.DataFrame(single_Q))\n",
    "\n",
    "    req_count_prob = utility.solve_CTMC(single_Q)\n",
    "    req_df = pd.DataFrame(data = {\n",
    "        'req_count': [s[0] for s in single_coder.get_state_list()],\n",
    "        'req_count_prob': req_count_prob,\n",
    "    })\n",
    "\n",
    "    # if 0 instances, any value for request count over 0 causes transition to 1 instances\n",
    "    if ready_inst_count == 0:\n",
    "        new_order_vals = [0, 1]\n",
    "        new_order_probs_zero = req_df['req_count_prob'][req_df['req_count'] == 0][0]\n",
    "        new_order_probs_one = 1 - new_order_probs_zero\n",
    "        new_order_probs = [new_order_probs_zero, new_order_probs_one]\n",
    "    else:\n",
    "        # calculate measure concurrency distribution\n",
    "        avg_count = model_config['stable_conc_avg_count']\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        req_count_averaged_vals, req_count_averaged_probs = utility.get_averaged_distribution(vals=req_df['req_count'], probs=req_df['req_count_prob'], avg_count=avg_count)\n",
    "        print(f\"new order calculation took {time.time() - start_time} seconds for {ready_inst_count} instances\")\n",
    "\n",
    "        # calculate probability of different ordered instance count\n",
    "        new_order_vals, new_order_probs = general_model.get_new_order_dist(req_count_averaged_vals, req_count_averaged_probs, model_config)\n",
    "\n",
    "    # plot the result\n",
    "    # plt.figure(figsize=(8,4))\n",
    "    # plt.bar(new_order_val, new_order_prob, width=1)\n",
    "\n",
    "    # now calculate probs according to number of ordered instances\n",
    "    for ordered_inst_count in range(0, model_config['max_container_count']+1):\n",
    "        # get idx of the \"from\" state\n",
    "        from_state_idx = general_state_coder.to_idx(state=(ordered_inst_count, ready_inst_count))\n",
    "\n",
    "        # calculate probability of number of ready instances\n",
    "        next_ready_vals, next_ready_probs = general_model.get_trans_probabilities(ready_count=ready_inst_count, ordered_count=ordered_inst_count, config=model_config)\n",
    "\n",
    "        # calculate probability for all combinations of \"to\" states\n",
    "        for new_order_idx, next_ready_idx in itertools.product(range(len(new_order_vals)), range(len(next_ready_vals))):\n",
    "            new_order_val = new_order_vals[new_order_idx]\n",
    "            new_order_prob = new_order_probs[new_order_idx]\n",
    "            next_ready_val = next_ready_vals[next_ready_idx]\n",
    "            next_ready_prob = next_ready_probs[next_ready_idx]\n",
    "\n",
    "            to_state_idx = general_state_coder.to_idx(state=(new_order_val, next_ready_val))\n",
    "            general_Q[from_state_idx, to_state_idx] = new_order_prob * next_ready_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# when everything is fixed, this should all be ones (almost, because of rounding errors)\n",
    "np.where((general_Q.sum(axis=1)-1) > 1e-6)"
   ]
  }
 ]
}